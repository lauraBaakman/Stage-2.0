%!TEX root = paper.tex

%General Idea
	We use our shape adaptive kernels in combination with the Modified Breiman Estimator introduced by \textcite{wilkinson1995dataplot}.
	%Pilot Densities
	The grid that the pilot densities are computed on \todo{Iets over hoe het grid bepaald wordt, of de standaard grootte van het grid.} 
	%General bandwidth
	We choose to use the method proposed by \textcite{ferdosi2011comparison} for computing the general bandwidth because of its lower complexity, compared to the method used by \textcite{wilkinson1995dataplot}. 
	%Local bandwidths
	We have empirically determined \todo{hoe hebben we dat vastgesteld} that using \varMBESensitivityParam = \todo{Een of andere waarde} works best in our case. 
	%Final densities
	The final densities are estimated according to \cref{eq:1:adaptiveKernelEstimateWithLocalBandwidths} with a reshaped and scaled Epanechnikov kernel. The Epanechnikov kernel reshaped with the matrix \varCovarianceMatrix is defined as:
	\todo[inline]{Ik verwacht dat hier nog een factor bij moet}
	\begin{equation}\label{eq:1:epanechnikovKernelWithCovarianceMatrix}
		\varKernel[\varEpan]{\varPattern} = 
		\begin{cases}
			? * \frac{\varDim + 2}{2\varUnitSphere{\varDim}} \left( 1 - \varPattern \varCovarianceMatrix \varPattern \right) & \text{if } \varPattern \cdot \varPattern < 1\\
			0 & \text{otherwise.}
		\end{cases}
	\end{equation}
	The matrix \varCovarianceMatrix determines the shape of the kernel. The shape of the kernel is determined based on the neighborhood of the pattern, \varPattern, whose density we are estimating. 

% The shape matrix

	% Finding the neighbours
	We determine the neighbors of \varPattern with the \KNNK nearest neighbors algorithm (\KNN) with Euclidean distance. This approach is used rather than a fixed-radius neighborhood to ensure that independent of the sparsity of the data the kernel shape is always based on a reasonable number of data points. 
	% Choosing K
	Furthermore using \KNN instead of the fixed-radius approach allows us to choose $\KNNK$ in such a way that the number of patterns the \varCovarianceMatrix is based on is greater than \varDim. This which makes it extremely improbable that the covariance matrix of the neighborhood is singular. We follow \citeauthor{silverman1986density}'s \cite{silverman1986density} recommendation of choosing $k = \sqrt{\varNumPatterns}$. To ensure that even the \KNNK computed for small high-dimensional data sets satisfies $\KNNK \geq \varDim$ we use
	\begin{equation*}
	\KNNK = \max\left(\left\lfloor \sqrt{\varNumPatterns} \right\rceil,\, \varDim \right).	
	\end{equation*}
	We let \varNeighborhood{\varPattern} denote the union of \varPattern and its \KNNK neighbors, the basic shape of the kernel used for \varPattern is then given by the biased covariance matrix of \varNeighborhood{\varPattern}.

	To allow the density estimation of each pattern to be influenced by an equal area, before the application of the smoothing factor $\varBandwidth\varLocalBandwidth{i}$, the basic shapes of the kernels need to be scaled. To that end we use the eigenellipse, the ellipse defined by the eigenvectors and eigenvalues of \varCovarianceFunction{\varNeighborhood{\varPattern}}. We scale the covariance matrix with the factor \varScalingFactor. For the Gaussian kernel this factor is defined as:
	\begin{equation}
		\varScalingFactor_{\varGaussian} = \frac{\varBandwidth^2}{\varGeometricMeanFunction{\sqrt{\lambda_1}, \dotsc, \sqrt{\lambda_\varDim}}},
	\end{equation}	
	where $\lambda_j$ denotes the $j$th eigenvalue of the $j$th eigenvector of \varCovarianceMatrix. 
	\todo{Klopt dit? Wiskundig gezien en met wat we weten van de standard gaussian}
	This reduces to $\varScalingFactor = h\sqrt{h}$ for a standard Gaussian kernel. If the Epanechnikov kernel is used the scaling factor is:
	\begin{equation}
		\varScalingFactor_{\varEpan} = ? 
	\end{equation}
	The scaling factors ensure that the shape-adapted covariance matrix has the same scale as the covariance matrix that is implicitly used in the Modified Breiman Estimator with a Gaussian kernel.